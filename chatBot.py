import requests
import json
from typing import Dict, Any

# --- Configuration for the ML Endpoint ---
# NOTE: Replace this with the actual URL of your deployed model (e.g., on Azure ML, AWS SageMaker, or a private server)
ML_ENDPOINT_URL = "https://api.your-ml-platform.com/v1/models/code-generator:predict" 
API_KEY = "YOUR_SECURE_API_KEY_HERE" 

def generate_content_from_ml_endpoint(prompt: str) -> Dict[str, Any]:
    """
    Simulates sending a request to a Machine Learning endpoint for content generation
    and handles the response.

    Args:
        prompt: The text instruction (the request) for the generative model.

    Returns:
        A dictionary containing the model's response data, or an error message.
    """
    
    # 1. Define the Request Payload (Input data for the model)
    # The structure depends entirely on your model and endpoint API design.
    payload = {
        "instances": [
            {
                "prompt": prompt,
                "max_tokens": 100,  # Limit the length of the generated code
                "temperature": 0.5  # Control randomness (0.0 is deterministic, 1.0 is creative)
            }
        ]
    }
    
    # 2. Define Request Headers (Authentication and Content Type)
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}",  # Secure way to pass credentials
        # Some platforms use 'x-api-key': API_KEY instead
    }

    print(f"Sending request to endpoint: {ML_ENDPOINT_URL}...")
    
    try:
        # 3. Send the POST Request
        response = requests.post(
            ML_ENDPOINT_URL,
            headers=headers,
            json=payload,
            timeout=30  # Wait up to 30 seconds for a response
        )

        # 4. Check for HTTP Errors
        response.raise_for_status() 
        
        # 5. Return the JSON response
        return response.json()

    except requests.exceptions.RequestException as e:
        # Handle connection errors, timeouts, and other request issues
        return {"error": f"Error connecting to ML endpoint: {e}"}
    except json.JSONDecodeError:
        # Handle cases where the server returns a non-JSON response
        return {"error": "Received an unreadable response from the server."}


if __name__ == '__main__':
    # --- Example Usage: Requesting a Python function ---
    
    code_prompt = "Write a Python function called 'calculate_bmi' that takes weight (kg) and height (m) and returns the BMI."

    # Get the generated response
    generation_response = generate_content_from_ml_endpoint(code_prompt)

    # --- Process and display the output ---
    
    if 'error' in generation_response:
        print("\n--- ERROR ---")
        print(generation_response['error'])
    else:
        # NOTE: This response structure is hypothetical; adjust based on your actual endpoint's JSON format.
        try:
            # Assuming the generated content is in the 'predictions' list
            generated_code = generation_response['predictions'][0]['generated_text'] 
            
            print("\n" + "=" * 50)
            print("âœ… SUCCESS: Code Generated by ML Endpoint")
            print("=" * 50)
            print(generated_code)
            print("=" * 50)
            
        except (KeyError, IndexError):
            print("\n--- WARNING ---")
            print("Successfully connected, but failed to parse the expected 'predictions' format.")
            print("Full response received:")
            print(json.dumps(generation_response, indent=2))
